{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@markdown tick the box below if you want detailed summary of the\n",
        "#@markdown process, else ignore it and run all\n",
        "Detailed_results = False #@param{type:\"boolean\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hGyn3rQoqp94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Install all required modules in a single go*"
      ],
      "metadata": {
        "id": "a31kSyp1Qcai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe opencv-python tensorflow numpy pandas matplotlib seaborn > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "ezT2B-yF2Kmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNZIP THE IMAGES**"
      ],
      "metadata": {
        "id": "olt9e83Br991"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown zip file name to be written below b/w the double quotes\n",
        "name = \"ar\" #@param {allow-input: true}\n",
        "!unzip -q {name}.zip"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uFDMfwLmsA8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORT ALL REQUIRED MODULES**"
      ],
      "metadata": {
        "id": "PAVg23QNQ6Px"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze2R-fOGP17E"
      },
      "outputs": [],
      "source": [
        "#@title Default title text\n",
        "import mediapipe as mp\n",
        "import cv2 as cv\n",
        "import csv\n",
        "import os\n",
        "import copy,itertools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All the paths of files/images and modules**"
      ],
      "metadata": {
        "id": "1jTjdiyySZAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "model_save_path = 'keypoint_classifier.hdf5'\n",
        "tflite_save_path = 'keypoint_classifier.tflite'\n",
        "csv_path = name+'.csv'\n",
        "image_folder = r\"data\""
      ],
      "metadata": {
        "id": "rYkHkCXKSYdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converts images-->hand_labels in csv format**"
      ],
      "metadata": {
        "id": "boQgwObrQFfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title .\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "        static_image_mode=True,\n",
        "        max_num_hands=1,\n",
        "        min_detection_confidence=0.7)\n",
        "\n",
        "\n",
        "def calc_landmark_list(image, landmarks):\n",
        "    image_height_y,image_width_x,_  = image.shape\n",
        "\n",
        "    landmark_point = []\n",
        "\n",
        "    # Keypoint\n",
        "    for landmark in landmarks.landmark: # 21/42  loops for learning 1/2 hands\n",
        "        landmark_x = min(int(landmark.x * image_width_x), image_width_x - 1)\n",
        "        landmark_y = min(int(landmark.y * image_height_y), image_height_y - 1)\n",
        "        # landmark_z = landmark.z\n",
        "\n",
        "        landmark_point.append([landmark_x, landmark_y])\n",
        "\n",
        "    return landmark_point\n",
        "\n",
        "def pre_process_landmark(landmark_list):\n",
        "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
        "\n",
        "    # Convert to relative coordinates(having a x=0 and y=0)\n",
        "    base_x, base_y = 0, 0\n",
        "    for index, landmark_point in enumerate(temp_landmark_list):\n",
        "        if index == 0:\n",
        "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
        "\n",
        "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
        "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
        "\n",
        "    # Convert to a one-dimensional list\n",
        "    temp_landmark_list = list(\n",
        "        itertools.chain.from_iterable(temp_landmark_list))\n",
        "\n",
        "    # Normalization\n",
        "    max_value = max(list(map(abs, temp_landmark_list)))\n",
        "\n",
        "    def normalize_(n):\n",
        "        return n / max_value\n",
        "\n",
        "    temp_landmark_list = list(map(normalize_, temp_landmark_list))#convert them from 0-1 range\n",
        "\n",
        "    return temp_landmark_list\n",
        "\n",
        "def logging_csv(number,landmark_list):\n",
        "    with open(csv_path, 'a', newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([number, *landmark_list])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "labels = []\n",
        "number = 0\n",
        "# os.chdir(os.listdir(image_folder)[0])\n",
        "for number,files in enumerate(sorted(os.listdir(image_folder))):\n",
        "    for img in os.listdir(os.path.join(image_folder,files)):\n",
        "        img_path = os.path.join(image_folder,files,img)\n",
        "        image = cv.imread(img_path)\n",
        "        image_bgr = cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
        "        results = hands.process(image_bgr)\n",
        "        if results.multi_hand_landmarks is not None:\n",
        "            for hand_landmarks, handedness in zip(results.multi_hand_landmarks,\n",
        "                                                  results.multi_handedness):\n",
        "\n",
        "                landmark_list = calc_landmark_list(image, hand_landmarks)\n",
        "\n",
        "                # Conversion to relative coordinates / normalized coordinates\n",
        "                pre_processed_landmark_list = pre_process_landmark(\n",
        "                    landmark_list)\n",
        "#\n",
        "                # Write to the dataset file\n",
        "                logging_csv(number, pre_processed_landmark_list)\n",
        "NUM_CLASSES = number+1"
      ],
      "metadata": {
        "id": "uFTukPEhQB-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD THE DATASETS FROM THE CSV FILE INTO *test* and *train* set**"
      ],
      "metadata": {
        "id": "wM8b4zIrRZyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_dataset = np.loadtxt(csv_path, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))\n",
        "y_dataset = np.loadtxt(csv_path, delimiter=',', dtype='int32', usecols=(0))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED) # you can change the train_size between 0.75-0.85 for good efficieny"
      ],
      "metadata": {
        "id": "pLIds3f_RpVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL CONSTRUCTION**"
      ],
      "metadata": {
        "id": "PtT1i82aTycf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    Input((21 * 2,)),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(5, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation='relu'),\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "#100 images per class is a minimum"
      ],
      "metadata": {
        "id": "iRPWmF4IT-p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL SUMMARY**"
      ],
      "metadata": {
        "id": "G_OOsmOXVkKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if Detailed_results:\n",
        "  model.summary()"
      ],
      "metadata": {
        "id": "WUHm4ZXfVjFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT CALLBACKS**"
      ],
      "metadata": {
        "id": "RXUrYm3tVs9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback to save the progress at intervals of time\n",
        "display(cp_callback := tf.keras.callbacks.ModelCheckpoint(\n",
        "    model_save_path, verbose=1, save_weights_only=False))\n",
        "# Callback for early stopping to avoid overfitting\n",
        "display(es_callback := tf.keras.callbacks.EarlyStopping(patience=20, verbose=1))"
      ],
      "metadata": {
        "id": "PlJmus9WVtME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COMPILE THE MODEL WITH REQUIRED SETTINGS**"
      ],
      "metadata": {
        "id": "IivAZXOFWh6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KrK_flhDWhHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL TRAINING**"
      ],
      "metadata": {
        "id": "TF8EO3Q7XEHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    #@markdown you can change these values based on the amount of classes present\n",
        "    epochs=30#@param\n",
        "    ,\n",
        "    batch_size=5#@param\n",
        "    ,validation_data=(X_test, y_test),\n",
        "    callbacks=[cp_callback, es_callback]\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YOA8SFdIXDMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**shows us the final accuracy of the model**"
      ],
      "metadata": {
        "id": "HMZ5sWBcYPIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if Detailed_results:\n",
        "  val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=64\n",
        "                                    )\n",
        "  # Loading the saved model\n",
        "  model = tf.keras.models.load_model(model_save_path)\n",
        "if Detailed_results:\n",
        "  #Inference test\n",
        "  predict_result = model.predict(np.array([X_test[0]]))\n",
        "  print(np.squeeze(predict_result))\n",
        "  print(np.argmax(np.squeeze(predict_result)))"
      ],
      "metadata": {
        "id": "KjS3wVnQYPXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONFUSION MATRIX**(grahical and tabular representation of loss & accuracy)"
      ],
      "metadata": {
        "id": "j6Z4ghXnY0ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def print_confusion_matrix(y_true, y_pred, report=True):\n",
        "    labels = sorted(list(set(y_true)))\n",
        "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
        "    ax.set_ylim(len(set(y_true)), 0)\n",
        "    plt.show()\n",
        "\n",
        "    if report:\n",
        "        print('Classification Report')\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "if Detailed_results:\n",
        "  Y_pred = model.predict(X_test)\n",
        "  y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "  print_confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "l7a8mjvWY0tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**saves the model as .tflite file in the specified path**"
      ],
      "metadata": {
        "id": "0Qg2L4iEZaN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(model_save_path, include_optimizer=False)\n",
        "[35]\n",
        "# Transform model (quantization)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quantized_model = converter.convert()\n",
        "\n",
        "with open(tflite_save_path, 'wb') as f:\n",
        "    f.write(tflite_quantized_model)"
      ],
      "metadata": {
        "id": "hAHgkYJbZaZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE TEST**"
      ],
      "metadata": {
        "id": "QN7kuWLWqSMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if Detailed_results:\n",
        "  interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Get I / O tensor\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "\n",
        "  interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]]))\n",
        "\n",
        "\n",
        "  # Inference implementation\n",
        "  interpreter.invoke()\n",
        "  tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "\n",
        "  print(np.squeeze(tflite_results))\n",
        "  print(np.argmax(np.squeeze(tflite_results)))"
      ],
      "metadata": {
        "id": "JqitD01PqV64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TO ZIP AND UNZIP THE IMAGES IN SYSTEM AND COLAB**"
      ],
      "metadata": {
        "id": "EFJfKS0nZ1cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -rq9 archive.zip <file-path> ------> in linux/mac\n",
        "\n",
        "# !Compress-Archive -Path 'directory-of-images' -DestinationPath 'archive.zip' -------> in windows(powershell)\n",
        "\n",
        "#labels should b written in lexographical order!"
      ],
      "metadata": {
        "id": "Ql1cGtrsZ0px"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}